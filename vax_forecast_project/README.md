# Vaccination Demand & Hospital Load ‚Äì Predictive Pipeline (FR)

Ce projet fournit un **squelette complet** (ingestion ‚ûú features ‚ûú mod√®les ‚ûú HTS ‚ûú optimisation stock)
pour **optimiser la couverture vaccinale** et **r√©duire la charge hospitali√®re**.

## üß± Contenu
- `src/data_ingestion.py` : import & nettoyage (INSEE, Sentinelles, OSCOUR, M√©t√©o, Vaccination).
- `src/feature_engineering.py` : assemblage hebdo, lags, moyennes mobiles, calendrier, normalisation per 100k.
- `src/models/baselines.py` : Prophet baseline par (r√©gion √ó √¢ge).
- `src/models/gbdt_demand.py` : LightGBM avec validation rolling-origin.
- `src/hts.py` : r√©conciliation hi√©rarchique Top-Down (proportions historiques).
- `src/opt/optimize_inventory.py` : Newsvendor & PL de r√©assort avec `pulp`.
- `src/train_pipeline.py` : run MLflow + export m√©triques & forecast r√©concili√©.
- `src/opt/plan_reassort.py` : plan de r√©assort hebdo √† partir des pr√©visions.
- `run_quickstart.py` : **d√©mo synth√©tique** end-to-end.

## üöÄ D√©marrage (d√©mo synth√©tique)
```bash
pip install -r requirements.txt
python run_quickstart.py
```

Les fichiers de sortie sont dans `data/processed/` :
- `features.parquet`
- `metrics_by_series.csv`
- `forecast_reconciled.parquet`
- `reassort_plan.csv`

## üîå Brancher vos vraies donn√©es
√âditez `conf/data_sources.yaml` et pointez vers vos CSV/API r√©els (SPF/Sentinelles/OSCOUR/INSEE/M√©t√©o-France/IQVIA).
Le sch√©ma attendu :
- **INSEE**: `region, age_band, population`
- **Sentinelles**: `date, region, incidence_per_100k`
- **OSCOUR**: `date, region, age_band, er_visits, admissions`
- **M√©t√©o**: `date, region, tmean`
- **Vaccination**: `date, region, age_band, doses`
- **R√©gions**: `region, region_name`

> Astuce : gardez des **codes courts** (`IDF`, `ARA`, ‚Ä¶) harmonis√©s partout.

## üß™ Validation & m√©triques
- `SMAPE` et `MAE` par (r√©gion √ó √¢ge).
- Validation **rolling-origin** (simule les pr√©visions semaine apr√®s semaine).

## üß© HTS (Hi√©rarchique)
D√©monstration **Top-Down**. Pour MinT, utilisez des lib sp√©cialis√©es ou impl√©mentez la covariance des erreurs.

## üè≠ Optimisation stock
- Newsvendor (fractile) pour dimensionner les seuils.
- **PL** via `pulp` pour allouer la capacit√© totale par r√©gion en contr√¥lant le risque de rupture.

## üìù Notes
- Prophet: baseline robuste sur saisonnalit√©s.
- LightGBM: capture non-lin√©arit√©s (incidence/m√©t√©o/retards).
- Les intervalles pr√©dictifs et les sc√©narios m√©t√©o/incidence sont √† int√©grer pour la prod.

## üîí Conformit√©
Travailler sur **agr√©gats hebdo** (r√©gion √ó √¢ge) ; pas d‚ÄôIPD. Journalisez sources et fraicheur des donn√©es.

---

_Fait pour servir de point de d√©part rapide ; ajoutez vos dashboards (Metabase/Superset/PowerBI) et votre infra (dbt/Airflow/MLflow Registry)._

## üîó Sources Open Data (branch√©es)

- **Sentinelles ‚Äì incidence hebdo** (Opendatasoft): `https://public.opendatasoft.com/api/records/1.0/download/?dataset=healthref-france-sentinelles-weekly&format=csv`
- **SurSaUD / OSCOUR ‚Äì urgences grippe** (ODiSSe): `https://odisse.santepubliquefrance.fr/api/records/1.0/download/?dataset=grippe-passages-aux-urgences-et-actes-sos-medecins-france&format=csv`
- **INSEE ‚Äì POP1A 2022** (CSV direct): `https://www.insee.fr/fr/statistiques/fichier/8581810/pop1a-2022.csv`
- **ODR√â ‚Äì Temp√©ratures quotidiennes r√©gionales**: `https://odre.opendatasoft.com/api/records/1.0/download/?dataset=temperature-quotidienne-regionale&format=csv`

### ‚ñ∂Ô∏è Importer les vraies donn√©es
```bash
# 1) Installer deps si pas fait
pip install -r requirements.txt  requests

# 2) T√©l√©charger et normaliser
python -c "from src.download_open_data import run_all; print(run_all())"

# 3) (Re)construire les features et entra√Æner
python -c "from src.train_pipeline import run_pipeline; print(run_pipeline())"
```

## üìä Superset & Metabase (exemples)
### Superset
```bash
cd dashboards/superset
docker compose up -d
# Ouvre http://localhost:8088 (admin/admin), ajoute une DB (DuckDB/SQLite/Postgres),
# puis 'Datasets' -> ajoutez data/processed/features.parquet (via un connecteur ou apr√®s import en SQL).
```

### Metabase
```bash
cd dashboards
docker compose -f metabase-docker-compose.yaml up -d
# Ouvre http://localhost:3000, cr√©ez l'admin, connectez votre base (DuckDB/Postgres),
# et cr√©ez des cartes √† partir de 'features.parquet' (si expos√© via un connecteur) ou tables import√©es.
```
